{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9pPoIQgApH9",
        "outputId": "7072db0a-f888-4c13-ece3-7cf1704dde66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "id": "F9pPoIQgApH9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          }
        },
        "id": "Em6ckbzICRGp",
        "outputId": "0e7a5ffb-d85c-4841-9e96-41067d33df32"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1a7d721d-0ea1-43d0-8e6e-eee122172da4\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1a7d721d-0ea1-43d0-8e6e-eee122172da4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-132-24b7de80b916>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#this will prompt you to upload the kaggle.json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m   \"\"\"\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    116\u001b[0m   result = _output.eval_js(\n\u001b[1;32m    117\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[0;32m--> 118\u001b[0;31m           input_id=input_id, output_id=output_id))\n\u001b[0m\u001b[1;32m    119\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.upload() #this will prompt you to upload the kaggle.json"
      ],
      "id": "Em6ckbzICRGp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2Yl6a7vCWxG",
        "outputId": "31467bc5-b8af-4388-bc96-595eb03153f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 69 Aug 21 12:24 kaggle.json\n",
            "/content\n",
            "ref                                                    title                                             size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
            "-----------------------------------------------------  -----------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
            "deepcontractor/mcdonalds-india-menu-nutrition-facts    McDonald's India : Menu Nutrition Dataset          5KB  2022-07-28 17:08:52           2085        107  0.9411765        \n",
            "iamsouravbanerjee/house-rent-prediction-dataset        House Rent Prediction Dataset                     82KB  2022-08-20 13:49:03           2094         91  1.0              \n",
            "arianazmoudeh/airbnbopendata                           Airbnb Open Data                                  10MB  2022-08-01 15:58:10           1495         52  0.9705882        \n",
            "ericsims/world-cheese-awards-worlds-cheesiest-dataset  World Cheese Awards (World's Cheesiest Dataset)   49KB  2022-08-17 03:11:43            228         24  1.0              \n",
            "tarundalal/100-richest-people-in-world                 100 Richest People In World                        3KB  2022-08-17 14:30:02            167         24  1.0              \n",
            "adhithyasrinivasan/worldwide-monkeypox-daily-dataset   Worldwide Monkeypox Daily Dataset                 30KB  2022-08-14 12:11:01            344         22  0.9411765        \n",
            "debashis74017/stock-market-index-data-india-1990-2022  Stock Market Index Data India (1990 - 2022)        2MB  2022-08-15 22:45:10            389         21  1.0              \n",
            "infamouscoder/dataset-netflix-shows                    Dataset: NetFlix Shows                             1MB  2022-08-11 05:05:41            679         40  1.0              \n",
            "himanshunakrani/student-study-hours                    Student Study Hours                               276B  2022-07-20 13:17:29           2423         78  1.0              \n",
            "vanvalkenberg/historicalweatherdataforindiancities     Weather data Indian cities (1990 to 2022)        504KB  2022-07-29 11:13:42            626         29  1.0              \n",
            "ruchi798/data-science-job-salaries                     Data Science Job Salaries                          7KB  2022-06-15 08:59:12          22438        694  1.0              \n",
            "ashvanths/toprated-tmdb-movies                         Top-Rated TMDB Movies                              1MB  2022-08-15 06:49:17            519         28  1.0              \n",
            "dansbecker/melbourne-housing-snapshot                  Melbourne Housing Snapshot                       451KB  2018-06-05 12:52:24          93484       1126  0.7058824        \n",
            "reihanenamdari/breast-cancer                           Breast Cancer                                     43KB  2022-08-08 19:25:55            559         34  1.0              \n",
            "zzettrkalpakbal/full-filled-brain-stroke-dataset       Brain stroke prediction dataset                   52KB  2022-07-16 09:57:08           2776         80  0.9705882        \n",
            "tigboatnc/amazon-all-categories-best-sellers-reviews   Amazon All Categories Best Sellers + Reviews       3MB  2022-08-10 00:26:09            627         25  1.0              \n",
            "nancyalaswad90/diamonds-prices                         Diamonds Prices                                  711KB  2022-07-09 14:59:21           2747        111  1.0              \n",
            "datasnaek/youtube-new                                  Trending YouTube Video Statistics                201MB  2019-06-03 00:56:47         182946       4640  0.7941176        \n",
            "surajjha101/countries-olympics-medals-since-1896       Countries Olympics Medals since 1896               4KB  2022-07-27 06:39:06           1102         55  1.0              \n",
            "zynicide/wine-reviews                                  Wine Reviews                                      51MB  2017-11-27 17:08:04         164771       3356  0.7941176        \n"
          ]
        }
      ],
      "source": [
        "!ls -lha kaggle.json\n",
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!pwd\n",
        "!kaggle datasets list"
      ],
      "id": "X2Yl6a7vCWxG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUOk-JbTCW4P",
        "outputId": "68f50c87-304e-440c-c130-2d74fd05eedd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "facial-expression-dataset-image-folders-fer2013.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d astraszab/facial-expression-dataset-image-folders-fer2013"
      ],
      "id": "DUOk-JbTCW4P"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_tiH6quKu4-",
        "outputId": "c8a8eefc-0ee5-4d29-9474-7fcbbc0943d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  facial-expression-dataset-image-folders-fer2013.zip\n",
            "replace data/test/0/32298.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!unzip facial-expression-dataset-image-folders-fer2013.zip"
      ],
      "id": "Z_tiH6quKu4-"
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "im = Image.open('data/train/0/0.png')\n",
        "# width, height = im.size\n",
        "print(im.size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nm05CRY9soS9",
        "outputId": "ebf0d172-c244-41d3-dfd9-f0f4e2d5c462"
      },
      "id": "nm05CRY9soS9",
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(48, 48)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "54349ec1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import itertools # To join a list of lists\n",
        "import matplotlib.pyplot as plt # For general plotting\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "# Utility to visualize PyTorch network and shapes\n",
        "from torchsummary import summary\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "# Set seed to generate reproducible \"pseudo-randomness\" (handles scipy's \"randomness\" too)\n",
        "# random_seed = 7\n",
        "# np.random.seed(random_seed)\n",
        "# torch.manual_seed(7)\n",
        "\n",
        "plt.rc('font', size=22)          # controls default text sizes\n",
        "plt.rc('axes', titlesize=18)     # fontsize of the axes title\n",
        "plt.rc('axes', labelsize=18)     # fontsize of the x and y labels\n",
        "plt.rc('xtick', labelsize=14)    # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=14)    # fontsize of the tick labels\n",
        "plt.rc('legend', fontsize=16)    # legend fontsize\n",
        "plt.rc('figure', titlesize=22)   # fontsize of the figure title"
      ],
      "id": "54349ec1"
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "6ec1401b",
        "outputId": "0c6dd6be-43dd-42de-d3dd-b4b1ff58b15f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset has N = 28709 samples\n",
            "Test dataset has N = 3589 samples\n",
            "Validation dataset has N = 3589 samples\n",
            "torch.Size([8, 1, 48, 48])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x1440 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define how transform the data for processing in batches\n",
        "transform = transforms.Compose([\n",
        "        # transforms.Resize([280,280]), # Resizing the image\n",
        "        transforms.Grayscale(num_output_channels =1),\n",
        "        transforms.ToTensor(),\n",
        "        \n",
        "        transforms.Normalize(mean=0.5,std=0.5) # 0.5 due to how torchvision data is in range [0,1]\n",
        "    ])\n",
        "# Specify the batch size for SGD\n",
        "batch_size = 64\n",
        "\n",
        "# Use the ImageFolder class, which is a generic dataloder useful when images stored in labelled directories\n",
        "# Apply transform to images specified above, importantly converting \"ToTensor\"\n",
        "train_dataset = torchvision.datasets.ImageFolder(root='data/train', transform=transform)\n",
        "test_dataset = torchvision.datasets.ImageFolder(root='data/test', transform=transform)\n",
        "valid_dataset = torchvision.datasets.ImageFolder(root='data/val', transform=transform)\n",
        "\n",
        "classes = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
        "print(\"Train dataset has N = {} samples\".format(len(train_dataset)))\n",
        "print(\"Test dataset has N = {} samples\".format(len(test_dataset)))\n",
        "print(\"Validation dataset has N = {} samples\".format(len(valid_dataset)))\n",
        "\n",
        "# Define iterable for our newly created dataset and shuffle samples \n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "### Plotting stuff for visualization ###\n",
        "fig = plt.figure(figsize=(20,20))\n",
        "# Utility function to show an image\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    # img_np = img.cpu().numpy()\n",
        "    print(img[0].shape)\n",
        "    print(img[1].shape)\n",
        "    print(img[2].shape)\n",
        "    print(img.shape)\n",
        "    imshow(np.transpose(img[0].cpu().detach().numpy(), (1, 2, 0)))#np.transpose(img_np, (1, 2, 0))\n",
        "\n",
        "\n",
        "# Get some random training dataset images\n",
        "dataiter = iter(train_dataloader)\n",
        "# print(\"detailer shape\",dataiter.shape)\n",
        "# Extract a batch\n",
        "images, labels = dataiter.next()\n",
        "# images, labels = images.to(device), labels.to(device)\n",
        "print(images[:8].shape)\n",
        "# Show 8 images for display\n",
        "# num_display = 8\n",
        "# make_grid= torchvision.utils.make_grid(images[:num_display], num_rows=1)\n",
        "# print(\"make grid size\",make_grid.shape)\n",
        "# print(\"slice\", make_grid[0].shape)\n",
        "# imshow(make_grid)\n",
        "# # Print labels as a concatenated string\n",
        "# print(\"\\nLabels corresponding to randomly drawn images from the training set:\")\n",
        "# print(\" \".join(f'{classes[labels[j]]:5s}\\t' for j in range(num_display)))"
      ],
      "id": "6ec1401b"
    },
    {
      "cell_type": "code",
      "source": [
        "print(images.shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvzN0lrpuMVj",
        "outputId": "769a396a-3bf5-4bba-ca8f-55ad94a7b7b4"
      },
      "id": "zvzN0lrpuMVj",
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "56a28e44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "294e18e4-8b9c-470d-ada4-a23157791c01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 48, 48]             640\n",
            "         MaxPool2d-2           [-1, 64, 24, 24]               0\n",
            "            Conv2d-3          [-1, 128, 24, 24]          73,856\n",
            "       BatchNorm2d-4          [-1, 128, 24, 24]             256\n",
            "         MaxPool2d-5          [-1, 128, 12, 12]               0\n",
            "           Dropout-6          [-1, 128, 12, 12]               0\n",
            "            Conv2d-7          [-1, 256, 12, 12]         295,168\n",
            "       BatchNorm2d-8          [-1, 256, 12, 12]             512\n",
            "         MaxPool2d-9            [-1, 256, 6, 6]               0\n",
            "          Dropout-10            [-1, 256, 6, 6]               0\n",
            "           Conv2d-11            [-1, 512, 6, 6]       1,180,160\n",
            "      BatchNorm2d-12            [-1, 512, 6, 6]           1,024\n",
            "        MaxPool2d-13            [-1, 512, 3, 3]               0\n",
            "          Dropout-14            [-1, 512, 3, 3]               0\n",
            "           Conv2d-15           [-1, 1024, 3, 3]       4,719,616\n",
            "      BatchNorm2d-16           [-1, 1024, 3, 3]           2,048\n",
            "        MaxPool2d-17           [-1, 1024, 1, 1]               0\n",
            "          Dropout-18           [-1, 1024, 1, 1]               0\n",
            "           Linear-19                   [-1, 64]          65,600\n",
            "          Dropout-20                   [-1, 64]               0\n",
            "      BatchNorm1d-21                   [-1, 64]             128\n",
            "           Linear-22                   [-1, 64]           4,160\n",
            "          Dropout-23                   [-1, 64]               0\n",
            "      BatchNorm1d-24                   [-1, 64]             128\n",
            "           Linear-25                    [-1, 7]             455\n",
            "================================================================\n",
            "Total params: 6,343,751\n",
            "Trainable params: 6,343,751\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 4.03\n",
            "Params size (MB): 24.20\n",
            "Estimated Total Size (MB): 28.23\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Note that PyTorch uses NCHW (samples, channels, height, width) image format convention\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, in_channels, num_filters, out_classes, kernel_size=3):\n",
        "        super().__init__()\n",
        "        # Conv2D layer with 'same' padding so image retains shape\n",
        "        self.conv1 = nn.Conv2d(in_channels, num_filters, kernel_size, padding='same')\n",
        "        # self.conv2_bn = nn.BatchNorm2d(num_filters)\n",
        "        self.drop = nn.Dropout(p=0.25)\n",
        "        self.drop2 =nn.Dropout(p=0.2)\n",
        "        #self.dropout = \n",
        "        # MaxPooling layer with 2x2 kernel size and stride 2\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(num_filters, num_filters*2, kernel_size, padding='same')\n",
        "        self.conv2_bn = nn.BatchNorm2d(num_filters*2)\n",
        "        self.conv3 = nn.Conv2d(num_filters*2, num_filters*4, kernel_size, padding='same')\n",
        "        self.conv3_bn = nn.BatchNorm2d(num_filters*4)\n",
        "        self.conv4 = nn.Conv2d(num_filters*4, num_filters*8, kernel_size, padding='same')\n",
        "        self.conv4_bn = nn.BatchNorm2d(num_filters*8)\n",
        "        self.conv5 = nn.Conv2d(num_filters*8, num_filters*16, kernel_size, padding='same')\n",
        "        self.conv5_bn = nn.BatchNorm2d(num_filters*16)\n",
        "        # self.conv6 = nn.Conv2d(num_filters*16, num_filters*32, kernel_size, padding='same')\n",
        "        # self.conv6_bn = nn.BatchNorm2d(num_filters*32)\n",
        "        self.fc1 = nn.Linear(num_filters*16*1*1, num_filters)\n",
        "        self.fc1_norm= nn.BatchNorm1d(num_filters)\n",
        "        self.fc2 = nn.Linear(num_filters, num_filters)\n",
        "        self.fc2_norm= nn.BatchNorm1d(num_filters)\n",
        "        self.fc3 = nn.Linear(num_filters, out_classes)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Non-linear ReLU activations between convolutional layers\n",
        "        # Conv->ReLU->Pooling\n",
        "        # 280x280 image -> 140x140 after pooling\n",
        "        # print(\"x shape is: \", x.shape)\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        # print(x.shape)\n",
        "        # 140x140 feature map -> 70x70 after pooling\n",
        "        x = self.drop(self.pool(self.conv2_bn(F.relu(self.conv2(x)))))\n",
        "        # print(x.shape)\n",
        "        # Flatten all dimensions except batch (start_dim=1)\n",
        "        x = self.drop(self.pool(self.conv3_bn(F.relu(self.conv3(x)))))\n",
        "        # print(x.shape)\n",
        "        x = self.drop(self.pool(self.conv4_bn(F.relu(self.conv4(x)))))\n",
        "        x = self.drop(self.pool(self.conv5_bn(F.relu(self.conv5(x)))))\n",
        "        # x = self.drop(self.pool(self.conv6_bn(F.relu(self.conv6(x)))))\n",
        "        # print(\"x before flat\",x.shape)\n",
        "        x = torch.flatten(x, 1) \n",
        "        # print(x.shape)\n",
        "        x = self.fc1_norm(F.relu(self.drop(self.fc1(x))))\n",
        "        x = self.fc2_norm(F.relu(self.drop(self.fc2(x))))\n",
        "        x = self.fc3(x)\n",
        "        # print(\"shape is: \", self.conv1.size, self.conv2.size, self.conv3.size, self.fc1.size, self.fc2.size)\n",
        "        \n",
        "        return x\n",
        "\n",
        "input_channels = images.shape[1]\n",
        "model = ConvNet(input_channels, num_filters=64, out_classes=len(classes))\n",
        "\n",
        "# Dummy inputs so we can plot a summary of the neural network's architecture and no. of parameters\n",
        "model=model.to(device)\n",
        "summary(model, input_size=(input_channels, 48, 48))\n",
        "\n",
        "# Double check if model exists\n",
        "# if os.path.exists(os.getcwd() + '/cnn_fmd.pth'):\n",
        "#     model.load_state_dict(torch.load(os.getcwd() + '/cnn_fmd.pth'))\n",
        "#     print(\"Loaded model from disk!\")\n"
      ],
      "id": "56a28e44"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTe-UYFiV2vo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e202a8a-f923-434d-8174-f5f45e11185d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.145180  [    0/28709]\n",
            "loss: 1.993859  [  640/28709]\n",
            "loss: 2.034746  [ 1280/28709]\n",
            "loss: 1.890343  [ 1920/28709]\n",
            "loss: 1.997593  [ 2560/28709]\n",
            "loss: 1.977610  [ 3200/28709]\n",
            "loss: 2.031662  [ 3840/28709]\n",
            "loss: 1.963867  [ 4480/28709]\n",
            "loss: 2.154509  [ 5120/28709]\n",
            "loss: 1.926085  [ 5760/28709]\n",
            "loss: 1.961634  [ 6400/28709]\n",
            "loss: 2.105683  [ 7040/28709]\n",
            "loss: 2.028382  [ 7680/28709]\n",
            "loss: 2.020552  [ 8320/28709]\n",
            "loss: 1.950358  [ 8960/28709]\n",
            "loss: 1.881876  [ 9600/28709]\n",
            "loss: 1.917986  [10240/28709]\n",
            "loss: 2.022004  [10880/28709]\n",
            "loss: 1.949697  [11520/28709]\n",
            "loss: 2.045978  [12160/28709]\n",
            "loss: 2.036646  [12800/28709]\n",
            "loss: 1.964605  [13440/28709]\n",
            "loss: 1.781789  [14080/28709]\n",
            "loss: 1.836921  [14720/28709]\n",
            "loss: 1.940383  [15360/28709]\n",
            "loss: 1.927135  [16000/28709]\n",
            "loss: 1.951290  [16640/28709]\n",
            "loss: 2.016847  [17280/28709]\n",
            "loss: 1.960751  [17920/28709]\n",
            "loss: 1.923481  [18560/28709]\n",
            "loss: 1.873003  [19200/28709]\n",
            "loss: 1.952561  [19840/28709]\n",
            "loss: 1.862387  [20480/28709]\n",
            "loss: 1.953987  [21120/28709]\n",
            "loss: 1.918116  [21760/28709]\n",
            "loss: 1.984318  [22400/28709]\n",
            "loss: 1.829165  [23040/28709]\n",
            "loss: 1.839354  [23680/28709]\n",
            "loss: 1.900867  [24320/28709]\n",
            "loss: 1.818051  [24960/28709]\n",
            "loss: 1.904400  [25600/28709]\n",
            "loss: 1.846234  [26240/28709]\n",
            "loss: 1.925756  [26880/28709]\n",
            "loss: 1.866883  [27520/28709]\n",
            "loss: 1.799709  [28160/28709]\n",
            "\n",
            "Test Error\n",
            "\tAccuracy: 32.5%, Avg loss: 1.769270 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.737620  [    0/28709]\n",
            "loss: 1.856783  [  640/28709]\n",
            "loss: 1.914038  [ 1280/28709]\n",
            "loss: 1.894870  [ 1920/28709]\n",
            "loss: 1.818078  [ 2560/28709]\n",
            "loss: 1.904037  [ 3200/28709]\n",
            "loss: 1.791744  [ 3840/28709]\n",
            "loss: 1.871943  [ 4480/28709]\n",
            "loss: 1.732728  [ 5120/28709]\n",
            "loss: 1.806785  [ 5760/28709]\n",
            "loss: 1.901744  [ 6400/28709]\n",
            "loss: 1.811659  [ 7040/28709]\n",
            "loss: 1.781480  [ 7680/28709]\n",
            "loss: 1.877477  [ 8320/28709]\n",
            "loss: 1.798159  [ 8960/28709]\n",
            "loss: 1.751647  [ 9600/28709]\n",
            "loss: 1.635407  [10240/28709]\n",
            "loss: 1.874026  [10880/28709]\n",
            "loss: 1.925565  [11520/28709]\n",
            "loss: 1.742454  [12160/28709]\n",
            "loss: 1.671496  [12800/28709]\n",
            "loss: 1.820022  [13440/28709]\n",
            "loss: 1.713077  [14080/28709]\n",
            "loss: 1.616943  [14720/28709]\n",
            "loss: 1.619186  [15360/28709]\n",
            "loss: 1.816489  [16000/28709]\n",
            "loss: 1.824581  [16640/28709]\n",
            "loss: 1.669219  [17280/28709]\n",
            "loss: 1.625160  [17920/28709]\n",
            "loss: 1.686510  [18560/28709]\n",
            "loss: 1.719328  [19200/28709]\n",
            "loss: 1.698516  [19840/28709]\n",
            "loss: 1.700607  [20480/28709]\n",
            "loss: 1.778432  [21120/28709]\n",
            "loss: 1.592731  [21760/28709]\n",
            "loss: 1.680254  [22400/28709]\n",
            "loss: 1.576200  [23040/28709]\n",
            "loss: 1.908156  [23680/28709]\n",
            "loss: 1.804608  [24320/28709]\n",
            "loss: 1.733427  [24960/28709]\n",
            "loss: 1.729125  [25600/28709]\n",
            "loss: 1.578002  [26240/28709]\n",
            "loss: 1.793372  [26880/28709]\n",
            "loss: 1.672472  [27520/28709]\n",
            "loss: 1.762548  [28160/28709]\n",
            "\n",
            "Test Error\n",
            "\tAccuracy: 39.5%, Avg loss: 1.605742 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.689291  [    0/28709]\n",
            "loss: 1.551094  [  640/28709]\n",
            "loss: 1.720004  [ 1280/28709]\n",
            "loss: 1.557066  [ 1920/28709]\n",
            "loss: 1.507683  [ 2560/28709]\n",
            "loss: 1.662378  [ 3200/28709]\n",
            "loss: 1.639605  [ 3840/28709]\n",
            "loss: 1.727295  [ 4480/28709]\n",
            "loss: 1.512832  [ 5120/28709]\n",
            "loss: 1.543448  [ 5760/28709]\n",
            "loss: 1.424728  [ 6400/28709]\n",
            "loss: 1.677201  [ 7040/28709]\n",
            "loss: 1.611007  [ 7680/28709]\n",
            "loss: 1.703653  [ 8320/28709]\n",
            "loss: 1.815457  [ 8960/28709]\n",
            "loss: 1.639764  [ 9600/28709]\n",
            "loss: 1.539120  [10240/28709]\n",
            "loss: 1.545962  [10880/28709]\n",
            "loss: 1.410916  [11520/28709]\n",
            "loss: 1.436282  [12160/28709]\n",
            "loss: 1.920272  [12800/28709]\n",
            "loss: 1.602021  [13440/28709]\n",
            "loss: 1.680105  [14080/28709]\n",
            "loss: 1.555015  [14720/28709]\n",
            "loss: 1.559711  [15360/28709]\n",
            "loss: 1.578340  [16000/28709]\n",
            "loss: 1.510160  [16640/28709]\n",
            "loss: 1.816537  [17280/28709]\n",
            "loss: 1.609063  [17920/28709]\n",
            "loss: 1.610572  [18560/28709]\n",
            "loss: 1.620725  [19200/28709]\n",
            "loss: 1.872317  [19840/28709]\n",
            "loss: 1.515782  [20480/28709]\n",
            "loss: 1.670073  [21120/28709]\n",
            "loss: 1.486385  [21760/28709]\n",
            "loss: 1.612254  [22400/28709]\n",
            "loss: 1.312542  [23040/28709]\n",
            "loss: 1.395420  [23680/28709]\n",
            "loss: 1.472616  [24320/28709]\n",
            "loss: 1.478230  [24960/28709]\n",
            "loss: 1.424301  [25600/28709]\n",
            "loss: 1.486381  [26240/28709]\n",
            "loss: 1.502744  [26880/28709]\n",
            "loss: 1.460170  [27520/28709]\n",
            "loss: 1.502378  [28160/28709]\n",
            "\n",
            "Test Error\n",
            "\tAccuracy: 45.9%, Avg loss: 1.472283 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.801731  [    0/28709]\n",
            "loss: 1.585375  [  640/28709]\n",
            "loss: 1.498485  [ 1280/28709]\n",
            "loss: 1.496080  [ 1920/28709]\n",
            "loss: 1.546230  [ 2560/28709]\n",
            "loss: 1.522711  [ 3200/28709]\n",
            "loss: 1.395577  [ 3840/28709]\n",
            "loss: 1.497072  [ 4480/28709]\n",
            "loss: 1.358605  [ 5120/28709]\n",
            "loss: 1.246838  [ 5760/28709]\n",
            "loss: 1.471129  [ 6400/28709]\n",
            "loss: 1.581977  [ 7040/28709]\n",
            "loss: 1.732199  [ 7680/28709]\n",
            "loss: 1.630624  [ 8320/28709]\n",
            "loss: 1.489069  [ 8960/28709]\n",
            "loss: 1.537379  [ 9600/28709]\n",
            "loss: 1.534537  [10240/28709]\n",
            "loss: 1.703364  [10880/28709]\n",
            "loss: 1.567227  [11520/28709]\n",
            "loss: 1.501446  [12160/28709]\n",
            "loss: 1.428555  [12800/28709]\n",
            "loss: 1.335478  [13440/28709]\n",
            "loss: 1.403627  [14080/28709]\n",
            "loss: 1.531068  [14720/28709]\n",
            "loss: 1.367056  [15360/28709]\n",
            "loss: 1.690968  [16000/28709]\n",
            "loss: 1.411350  [16640/28709]\n",
            "loss: 1.552355  [17280/28709]\n",
            "loss: 1.564826  [17920/28709]\n",
            "loss: 1.471409  [18560/28709]\n",
            "loss: 1.379679  [19200/28709]\n",
            "loss: 1.551193  [19840/28709]\n",
            "loss: 1.603378  [20480/28709]\n",
            "loss: 1.403172  [21120/28709]\n",
            "loss: 1.369066  [21760/28709]\n",
            "loss: 1.507993  [22400/28709]\n",
            "loss: 1.441466  [23040/28709]\n",
            "loss: 1.282748  [23680/28709]\n",
            "loss: 1.386148  [24320/28709]\n",
            "loss: 1.472306  [24960/28709]\n",
            "loss: 1.476138  [25600/28709]\n",
            "loss: 1.527743  [26240/28709]\n",
            "loss: 1.189547  [26880/28709]\n",
            "loss: 1.509434  [27520/28709]\n",
            "loss: 1.492645  [28160/28709]\n",
            "\n",
            "Test Error\n",
            "\tAccuracy: 49.5%, Avg loss: 1.363923 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.707614  [    0/28709]\n",
            "loss: 1.381776  [  640/28709]\n",
            "loss: 1.646151  [ 1280/28709]\n",
            "loss: 1.392387  [ 1920/28709]\n",
            "loss: 1.509295  [ 2560/28709]\n",
            "loss: 1.406797  [ 3200/28709]\n",
            "loss: 1.557621  [ 3840/28709]\n",
            "loss: 1.437814  [ 4480/28709]\n",
            "loss: 1.436362  [ 5120/28709]\n",
            "loss: 1.356607  [ 5760/28709]\n",
            "loss: 1.469943  [ 6400/28709]\n",
            "loss: 1.404223  [ 7040/28709]\n",
            "loss: 1.401360  [ 7680/28709]\n",
            "loss: 1.504647  [ 8320/28709]\n",
            "loss: 1.532203  [ 8960/28709]\n",
            "loss: 1.248047  [ 9600/28709]\n",
            "loss: 1.515716  [10240/28709]\n",
            "loss: 1.369511  [10880/28709]\n",
            "loss: 1.752991  [11520/28709]\n",
            "loss: 1.470729  [12160/28709]\n",
            "loss: 1.455671  [12800/28709]\n",
            "loss: 1.222684  [13440/28709]\n",
            "loss: 1.413786  [14080/28709]\n",
            "loss: 1.526218  [14720/28709]\n",
            "loss: 1.363680  [15360/28709]\n",
            "loss: 1.352986  [16000/28709]\n",
            "loss: 1.448125  [16640/28709]\n",
            "loss: 1.463514  [17280/28709]\n",
            "loss: 1.289918  [17920/28709]\n",
            "loss: 1.428283  [18560/28709]\n",
            "loss: 1.301633  [19200/28709]\n",
            "loss: 1.423923  [19840/28709]\n",
            "loss: 1.470959  [20480/28709]\n",
            "loss: 1.288563  [21120/28709]\n",
            "loss: 1.462165  [21760/28709]\n",
            "loss: 1.501892  [22400/28709]\n",
            "loss: 1.354582  [23040/28709]\n",
            "loss: 1.330166  [23680/28709]\n",
            "loss: 1.328605  [24320/28709]\n",
            "loss: 1.300820  [24960/28709]\n",
            "loss: 1.327751  [25600/28709]\n",
            "loss: 1.336830  [26240/28709]\n",
            "loss: 1.362495  [26880/28709]\n",
            "loss: 1.648556  [27520/28709]\n",
            "loss: 1.576367  [28160/28709]\n",
            "\n",
            "Test Error\n",
            "\tAccuracy: 51.1%, Avg loss: 1.289839 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.624376  [    0/28709]\n",
            "loss: 1.498985  [  640/28709]\n",
            "loss: 1.416233  [ 1280/28709]\n",
            "loss: 1.476114  [ 1920/28709]\n",
            "loss: 1.507864  [ 2560/28709]\n",
            "loss: 1.277407  [ 3200/28709]\n",
            "loss: 1.163296  [ 3840/28709]\n",
            "loss: 1.252392  [ 4480/28709]\n",
            "loss: 1.572149  [ 5120/28709]\n",
            "loss: 1.379198  [ 5760/28709]\n",
            "loss: 1.323922  [ 6400/28709]\n",
            "loss: 1.363884  [ 7040/28709]\n",
            "loss: 1.400542  [ 7680/28709]\n",
            "loss: 1.244188  [ 8320/28709]\n",
            "loss: 1.346279  [ 8960/28709]\n",
            "loss: 1.261665  [ 9600/28709]\n",
            "loss: 1.468711  [10240/28709]\n",
            "loss: 1.526236  [10880/28709]\n",
            "loss: 1.492325  [11520/28709]\n",
            "loss: 1.335363  [12160/28709]\n",
            "loss: 1.384152  [12800/28709]\n",
            "loss: 1.560786  [13440/28709]\n",
            "loss: 1.375799  [14080/28709]\n",
            "loss: 1.193976  [14720/28709]\n",
            "loss: 1.236507  [15360/28709]\n",
            "loss: 1.183093  [16000/28709]\n",
            "loss: 1.389585  [16640/28709]\n",
            "loss: 1.463010  [17280/28709]\n",
            "loss: 1.334222  [17920/28709]\n",
            "loss: 1.256485  [18560/28709]\n",
            "loss: 1.182313  [19200/28709]\n",
            "loss: 1.414096  [19840/28709]\n",
            "loss: 1.171912  [20480/28709]\n",
            "loss: 1.464614  [21120/28709]\n",
            "loss: 1.396288  [21760/28709]\n",
            "loss: 1.370798  [22400/28709]\n",
            "loss: 1.369979  [23040/28709]\n",
            "loss: 1.321790  [23680/28709]\n",
            "loss: 1.263887  [24320/28709]\n",
            "loss: 1.396635  [24960/28709]\n",
            "loss: 1.421138  [25600/28709]\n",
            "loss: 1.261041  [26240/28709]\n",
            "loss: 1.123543  [26880/28709]\n",
            "loss: 1.296548  [27520/28709]\n",
            "loss: 1.478842  [28160/28709]\n",
            "\n",
            "Test Error\n",
            "\tAccuracy: 52.5%, Avg loss: 1.273505 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.163201  [    0/28709]\n",
            "loss: 1.132708  [  640/28709]\n",
            "loss: 1.237245  [ 1280/28709]\n",
            "loss: 1.189280  [ 1920/28709]\n",
            "loss: 1.282471  [ 2560/28709]\n",
            "loss: 1.438910  [ 3200/28709]\n",
            "loss: 1.125515  [ 3840/28709]\n",
            "loss: 1.346971  [ 4480/28709]\n",
            "loss: 1.417966  [ 5120/28709]\n",
            "loss: 1.316275  [ 5760/28709]\n",
            "loss: 1.415276  [ 6400/28709]\n",
            "loss: 1.180069  [ 7040/28709]\n",
            "loss: 1.444548  [ 7680/28709]\n",
            "loss: 1.274068  [ 8320/28709]\n",
            "loss: 1.409080  [ 8960/28709]\n",
            "loss: 1.322232  [ 9600/28709]\n",
            "loss: 1.267915  [10240/28709]\n",
            "loss: 1.483595  [10880/28709]\n",
            "loss: 1.258264  [11520/28709]\n",
            "loss: 1.296559  [12160/28709]\n",
            "loss: 1.161822  [12800/28709]\n",
            "loss: 1.124989  [13440/28709]\n",
            "loss: 1.209095  [14080/28709]\n",
            "loss: 1.208366  [14720/28709]\n",
            "loss: 0.989227  [15360/28709]\n",
            "loss: 1.369794  [16000/28709]\n",
            "loss: 1.177144  [16640/28709]\n",
            "loss: 1.381163  [17280/28709]\n",
            "loss: 1.334099  [17920/28709]\n",
            "loss: 1.151202  [18560/28709]\n",
            "loss: 1.117125  [19200/28709]\n",
            "loss: 1.345887  [19840/28709]\n",
            "loss: 1.264441  [20480/28709]\n",
            "loss: 1.343710  [21120/28709]\n",
            "loss: 1.018568  [21760/28709]\n",
            "loss: 1.177727  [22400/28709]\n",
            "loss: 1.238989  [23040/28709]\n",
            "loss: 1.389468  [23680/28709]\n",
            "loss: 1.325452  [24320/28709]\n",
            "loss: 1.257415  [24960/28709]\n",
            "loss: 1.302831  [25600/28709]\n",
            "loss: 1.442752  [26240/28709]\n",
            "loss: 1.210000  [26880/28709]\n",
            "loss: 1.327595  [27520/28709]\n",
            "loss: 1.057862  [28160/28709]\n",
            "\n",
            "Test Error\n",
            "\tAccuracy: 54.3%, Avg loss: 1.205338 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.118621  [    0/28709]\n",
            "loss: 1.261836  [  640/28709]\n",
            "loss: 1.243760  [ 1280/28709]\n",
            "loss: 1.265870  [ 1920/28709]\n",
            "loss: 1.329181  [ 2560/28709]\n",
            "loss: 1.348513  [ 3200/28709]\n",
            "loss: 1.162967  [ 3840/28709]\n",
            "loss: 1.481860  [ 4480/28709]\n",
            "loss: 1.283693  [ 5120/28709]\n",
            "loss: 1.091329  [ 5760/28709]\n",
            "loss: 1.226240  [ 6400/28709]\n",
            "loss: 1.309948  [ 7040/28709]\n",
            "loss: 1.229178  [ 7680/28709]\n",
            "loss: 1.364928  [ 8320/28709]\n",
            "loss: 1.476708  [ 8960/28709]\n",
            "loss: 1.193519  [ 9600/28709]\n",
            "loss: 1.341199  [10240/28709]\n",
            "loss: 1.280203  [10880/28709]\n",
            "loss: 1.236214  [11520/28709]\n",
            "loss: 1.254988  [12160/28709]\n",
            "loss: 1.377131  [12800/28709]\n",
            "loss: 1.306088  [13440/28709]\n",
            "loss: 1.106080  [14080/28709]\n",
            "loss: 1.254686  [14720/28709]\n",
            "loss: 1.335650  [15360/28709]\n",
            "loss: 1.339046  [16000/28709]\n",
            "loss: 1.226706  [16640/28709]\n",
            "loss: 1.269461  [17280/28709]\n",
            "loss: 1.103994  [17920/28709]\n",
            "loss: 1.460877  [18560/28709]\n",
            "loss: 1.182610  [19200/28709]\n",
            "loss: 1.278453  [19840/28709]\n",
            "loss: 1.311676  [20480/28709]\n",
            "loss: 1.454410  [21120/28709]\n",
            "loss: 1.059652  [21760/28709]\n",
            "loss: 1.409092  [22400/28709]\n",
            "loss: 1.233160  [23040/28709]\n",
            "loss: 1.138728  [23680/28709]\n",
            "loss: 1.321576  [24320/28709]\n",
            "loss: 1.032692  [24960/28709]\n",
            "loss: 1.194935  [25600/28709]\n",
            "loss: 1.273398  [26240/28709]\n",
            "loss: 1.147446  [26880/28709]\n",
            "loss: 1.069448  [27520/28709]\n",
            "loss: 1.221374  [28160/28709]\n",
            "\n",
            "Test Error\n",
            "\tAccuracy: 54.2%, Avg loss: 1.177479 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1.172026  [    0/28709]\n",
            "loss: 1.006389  [  640/28709]\n",
            "loss: 1.094919  [ 1280/28709]\n",
            "loss: 1.168848  [ 1920/28709]\n",
            "loss: 1.184736  [ 2560/28709]\n",
            "loss: 1.255491  [ 3200/28709]\n",
            "loss: 1.221959  [ 3840/28709]\n",
            "loss: 0.939525  [ 4480/28709]\n",
            "loss: 1.021755  [ 5120/28709]\n",
            "loss: 0.987327  [ 5760/28709]\n",
            "loss: 1.393420  [ 6400/28709]\n",
            "loss: 1.244908  [ 7040/28709]\n",
            "loss: 1.084294  [ 7680/28709]\n",
            "loss: 1.304552  [ 8320/28709]\n",
            "loss: 1.435057  [ 8960/28709]\n",
            "loss: 1.250086  [ 9600/28709]\n",
            "loss: 1.142527  [10240/28709]\n",
            "loss: 1.123585  [10880/28709]\n",
            "loss: 1.158066  [11520/28709]\n",
            "loss: 1.202002  [12160/28709]\n",
            "loss: 1.269424  [12800/28709]\n",
            "loss: 1.245187  [13440/28709]\n",
            "loss: 1.255522  [14080/28709]\n",
            "loss: 1.285021  [14720/28709]\n",
            "loss: 1.146761  [15360/28709]\n",
            "loss: 1.441395  [16000/28709]\n",
            "loss: 1.006296  [16640/28709]\n",
            "loss: 1.188596  [17280/28709]\n",
            "loss: 1.110256  [17920/28709]\n",
            "loss: 1.017331  [18560/28709]\n",
            "loss: 1.189584  [19200/28709]\n",
            "loss: 1.197573  [19840/28709]\n",
            "loss: 1.239678  [20480/28709]\n",
            "loss: 1.322834  [21120/28709]\n",
            "loss: 1.172806  [21760/28709]\n",
            "loss: 1.184127  [22400/28709]\n",
            "loss: 1.165065  [23040/28709]\n",
            "loss: 1.124514  [23680/28709]\n",
            "loss: 1.290444  [24320/28709]\n",
            "loss: 1.162448  [24960/28709]\n",
            "loss: 1.125114  [25600/28709]\n",
            "loss: 0.901964  [26240/28709]\n",
            "loss: 1.123778  [26880/28709]\n",
            "loss: 1.010259  [27520/28709]\n",
            "loss: 1.239476  [28160/28709]\n",
            "\n",
            "Test Error\n",
            "\tAccuracy: 57.0%, Avg loss: 1.138989 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.065540  [    0/28709]\n",
            "loss: 1.170874  [  640/28709]\n",
            "loss: 1.138250  [ 1280/28709]\n",
            "loss: 1.109159  [ 1920/28709]\n",
            "loss: 1.147390  [ 2560/28709]\n",
            "loss: 1.229294  [ 3200/28709]\n",
            "loss: 1.149728  [ 3840/28709]\n",
            "loss: 1.240243  [ 4480/28709]\n",
            "loss: 1.148909  [ 5120/28709]\n"
          ]
        }
      ],
      "source": [
        "def model_train_loader(model, dataloader, criterion, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction error\n",
        "        X=X.to(device)\n",
        "        y=y.to(device)\n",
        "        predictions = model(X)\n",
        "        loss = criterion(predictions, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Report loss every 10 batches\n",
        "        if batch % 10 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    \n",
        "            \n",
        "def model_test_loader(model, dataloader, criterion):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    # Tracking test loss (cross-entropy) and correct classification rate (accuracy)\n",
        "    test_loss, correct = 0, 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for X, y in dataloader:\n",
        "            X=X.to(device)\n",
        "            y=y.to(device)\n",
        "            predictions = model(X)\n",
        "            test_loss += criterion(predictions, y)\n",
        "            correct += (predictions.argmax(1) == y).type(torch.float).sum().item()\n",
        "            \n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"\\nTest Error\\n\\tAccuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "# The nn.CrossEntropyLoss() loss function automatically performs a log_softmax() to output\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# One is enough for my toy example...\n",
        "num_epochs = 200\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.00008)#, momentum=0.9, nesterov=True)\n",
        "\n",
        "for t in range(num_epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    model_train_loader(model, train_dataloader, criterion, optimizer)\n",
        "    model_test_loader(model, test_dataloader, criterion)\n",
        "    \n",
        "# Saving the model file 'cnn_fmd.pth' to my current working directory (cwd)\n",
        "print(\"Saving model to disk!\")\n",
        "torch.save(model.state_dict(), os.getcwd() + '/cnn_fmd.pth')\n",
        "# At filter size 64:\n",
        "#0.000008 epoch- 190 63-64% max   \n",
        "#0.00001 epoch - 110 62-64% max\n",
        "#0.00003 epoch - 60 63-64% max\n",
        "#0.00004 epoch - 40 63-64% max\n",
        "#0.00006 epoch - 30 63.5-64.3%\n",
        "#0.00008 epoch - 45 64.7%\n",
        "#0.0001"
      ],
      "id": "DTe-UYFiV2vo"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPool2D, Flatten,Dense,Dropout,BatchNormalization\n",
        "from tensorflow.keras.applications import VGG16, InceptionResNetV2\n",
        "from keras import regularizers\n",
        "from tensorflow.keras.optimizers import Adam,RMSprop,SGD,Adamax\n",
        "\n",
        "\n",
        "model= tf.keras.models.Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(48, 48,1)))\n",
        "model.add(Conv2D(64,(3,3), padding='same', activation='relu' ))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(128,(5,5), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "    \n",
        "model.add(Conv2D(512,(3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(512,(3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten()) \n",
        "model.add(Dense(256,activation = 'relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.25))\n",
        "    \n",
        "model.add(Dense(512,activation = 'relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Dense(7, activation='softmax'))\n",
        "\n",
        "model.compile(\n",
        "    optimizer = Adam(lr=0.0001), \n",
        "    loss='categorical_crossentropy', \n",
        "    metrics=['accuracy']\n",
        "  )"
      ],
      "metadata": {
        "id": "98UYGeo3mkRn"
      },
      "id": "98UYGeo3mkRn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(images.shape)"
      ],
      "metadata": {
        "id": "DdLMybPaob40"
      },
      "id": "DdLMybPaob40",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96dc7980"
      },
      "outputs": [],
      "source": [
        "### Plotting stuff ###\n",
        "fig = plt.figure(figsize=(20,20))\n",
        "\n",
        "# Get some random test dataset images\n",
        "dataiter = iter(test_dataloader)\n",
        "# Extract a batch\n",
        "images, labels = dataiter.next()\n",
        "# Show images\n",
        "imshow(torchvision.utils.make_grid(images[:num_display]))\n",
        "\n",
        "outputs = model(images)\n",
        "# Argmax take the most probable class\n",
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "# Print labels\n",
        "print(\"\\nGroundTruth:\\n\", \" \".join(f'{classes[labels[j]]:5s}\\t' for j in range(8)))\n",
        "\n",
        "print(\"\\nPredicted:\\n\", \" \".join(f'{classes[predicted[j]]:5s}\\t' for j in range(8)))"
      ],
      "id": "96dc7980"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d502a756"
      },
      "outputs": [],
      "source": [
        "correct = 0\n",
        "\n",
        "predicted_list = []\n",
        "labels_list = []\n",
        "\n",
        "# Since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    i = 0\n",
        "    for data in test_dataloader:\n",
        "        i += 1\n",
        "        images, labels = data\n",
        "        # Forward pass through network\n",
        "        outputs = model(images)\n",
        "        # Take most probable class\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        predicted_list.append(predicted.detach().numpy())\n",
        "        labels_list.append(labels.detach().numpy())\n",
        "        \n",
        "# Hacky conversions from a list of lists of arrays into joined numpy arrays\n",
        "predicted_list = np.array(list(itertools.chain.from_iterable(predicted_list)))\n",
        "labels_list = np.array(list(itertools.chain.from_iterable(labels_list)))\n",
        "# Count up number of labels and correct predictions\n",
        "correct = sum(predicted_list == labels_list)\n",
        "\n",
        "print(f\"Accuracy of the network on the test set images: {100 * correct // len(test_dataloader.dataset)} %\")\n",
        "\n",
        "print(\"Confusion Matrix (columns: True class, rows: Predicted class):\")\n",
        "conf_mat = confusion_matrix(labels_list, predicted_list);\n",
        "conf_display = ConfusionMatrixDisplay.from_predictions(labels_list, predicted_list, display_labels=classes, colorbar=False);\n",
        "plt.xlabel(\"True Labels\");\n",
        "plt.ylabel(\"Predicted Labels\");"
      ],
      "id": "d502a756"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "AUg21 6 17pmFinal Project.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}